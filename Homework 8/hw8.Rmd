---
title: Homework 8
author: Ansel George
output:
  pdf_document:
    latex_engine: xelatex
    highlight: tango
fontsize: 12pt
mainfont: LiberationSans
sansfont: LiberationSans
mathfont: LiberationSans
monofont: LiberationMono
---

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\argmin}{\operatornamewithlimits{arg\ min}}
\newcommand{\argmax}{\operatornamewithlimits{arg\ max}}

```{r, message=F}
library(glmnet)
library(mvtnorm)

set.seed(10)
```

# Part I: PCA & Penalized Regression

## Problem 1

### a)
*Explain the relationship between SVD, PCA, QR and eigenvalue decomposition.*

SVD: Singular value decomposition differs from PCA in that it exists for any
matrix, including non-square ones.

QR factorization exists for any matrix and decomposes a matrix $A$ into the
product $A=QR$, where $Q$ is an orthonormal matrix and $R$ is upper triangular.

An EVD, which only exists for invertible matrices, decomposes a matrix $A$ into
the products $A=V\Lambda V^T$, where $V$ is the set of eigenvectors and
$\Lambda$ a diagonal set of eigenvalues. If one were to multiply $\Lambda V^T$,
it would result in an upper-triangular matrix - $R$ in the formula for the $QR$
factorization. Also, because $V$ is a set of eigenvectors, they are by
definition orthogonal - $Q$ in terms of the $QR$ factorization.


### b)
*Implement Gram-Schmidt orthogonalization, and use it to perform QR, SVD and
PCA.*

Gram-Schmidt is poorly conditioned and in general is a stupid idea, especially
when iteratively computing QR factorizations to get the SVD and PCA, so I'll
use Householder reflections instead. They work by sequential reflections of the
original matrix --- an operation where the condition number is 1. The result is
that errors in $A$ aren't amplified over sequential operations, assuming $A$
represents real data that contains noise, etc. from measurement.

```{r}
HouseholderQR <- function(X) {
  m <- nrow(X)
  n <- ncol(X)

  if (m > n) {
    l <- n
  } else {
    l <- m
  }

  Q <- diag(m)
  R <- X

  for(i in 1:(l)) {
    v <- R[i:m, i]
    e <- rep(0, length(v))
    e[1] <- norm(v, "2") * sign(v[1])
    v <- v + e
    # if ((length(v) == 1) && (all.equal(v, 0) == T)) {
    #   v <- 0
    # } else {
    #   v <- v / norm(v, "2")
    # }
    v <- v / norm(v, "2")
    H <- diag(m)
    H[i:m, i:m] <- H[i:m, i:m] - 2*outer(v, v)
    R <- H %*% R
    Q <- Q %*% H
  }

  return(list(Q=Q,R=R))
}

GramSchmidtQR <- function(A) {
  m <- dim(A)[1]
  n <- dim(A)[2]

  Q <- matrix(0, nrow=m, ncol=n)
  R <- matrix(0, nrow=n, ncol=n)

  for (j in 1:n) {
    v <- A[,j]
    for (i in 1:j) {
      R[i,j] <- Q[,i] %*% A[,j]
      v <- v - R[i,j] * Q[,i]
    }
    R[j,j] <- norm(v, type="2")
    Q[,j] <- v / R[j,j]
  }

  return(list(Q=Q, R=R))
}
```

```{r}
n <- 10
p <- 8
A <- matrix(rnorm(n*p), nrow=n, ncol=p)
```

**QR:**

```{r}
res <- HouseholderQR(A)
Q <- res$Q
R <- res$R
```

```{r}
round(Q, 4) # round to make 0's more obvious
round(R, 4) # round to make 0's more obvious

all.equal(Q %*% t(Q), diag(dim(Q)[1])) # check that Q is orthogonal
all.equal(Q %*% R, A) # check that the QR evaluates to A
```


**QR and PCA:**

The PCA exists for the symmetric matrix $X$ where the mean of $X$ is centered
at 0, and the eigenvalues and eigenvectors can be found though the iterative
algorithm:

\begin{equation}
  \begin{split}
    X^{(t)} = Q^{(t)} R^{(t)} \\
    X^{(t+1)} = R^{(t)} Q^{(t)}
  \end{split}
\end{equation}

The PCA of X takes the form:

\begin{equation}
  X = V \Lambda V^T
\end{equation}

where $V$ is the set of eigenvectors and $\Lambda$ the diagonal of
corresponding eigenvalues.

The iteration has the property that:

\begin{align}
  X^{(t+1)} &= R^{(t)} Q^{(t)} \\
            &= {Q^{(t)}}^T Q^{(t)} R^{(t)} Q^{(t)} \\
            &= {Q^{(t)}}^T X^{(t)} Q^{(t)} \\
  X^{(t+2)} &= R^{(t+1)} Q^{(t+1)} \\
            &= {Q^{(t+1)}}^T Q^{(t+1)} R^{(t+1)} Q^{(t+1)} \\
            &= {Q^{(t+1)}}^T X^{(t+1)} Q^{(t+1)} \\
            &= {Q^{(t+1)}}^T {Q^{(t)}}^T X^{(t)} Q^{(t)} Q^{(t+1)} \\
  &\vdots \\
  X^{(n)} &= {\prod^n Q^{(i)}}^T X^{(t=1)} \prod^n Q^{(i)}
\end{align}


```{r}
PCAfromQR <- function(A, QR, reps=100) {
  m <- dim(A)[1]
  n <- dim(A)[2]

  # Center the data and compute the covariance matrix
  A_center <- A - apply(A, 2, mean)
  A <- apply(A, 1, function(x) {x - colMeans(A)})
  A <- t(A)
  X <- cov(A)

  V <- diag(1, dim(X)[1]);
  L <- X

  for(i in 1:reps) {
    res <- QR(L)
    Q <- res$Q
    R <- res$R
    V <- V %*% Q
    L <- R %*% Q
  }

  return(list(V=V, L=L))
}
```

```{r}
# res <- PCAfromQR(A, HouseholderQR, reps=500)
res <- PCAfromQR(A, GramSchmidtQR, reps=500)
V <- res$V
L <- res$L

# Compare values with the built-in `prcomp` function:
res <- prcomp(A)

# Note that A is not full rank, so the last two eigenvectors and
# eigenvectors are nonsense. 
diag(L)
all.equal.numeric(diag(L), res$sdev^2)

V
V_adj <- V %*% diag(round(res$rotation/V)[1,]) # adjust signs on eigenvectors
all.equal(matrix(res$rotation, ncol=ncol(res$rotation)), V_adj)
```


**QR and SVD:**

```{r}
SVDfromQR <- function(X, QR, reps=100) {
  nc <- ncol(X)
  nr <- nrow(X)

  L1 <- X %*% t(X)
  U <- diag(1, dim(L1)[1]);
  for(i in 1:reps) {
    res <- QR(L1)
    Q <- res$Q
    U <- U %*% Q
    L1 <- res$R %*% Q
  }

  L2 <- t(X) %*% X
  V <- diag(1, dim(L2)[1]);
  for(i in 1:reps) {
    res <- QR(L2)
    Q <- res$Q
    V <- V %*% Q
    L2 <- res$R %*% Q
  }

  # Pad the sigma matrix or subset it based on the dimensions of the input
  # matrix.
  if (nc > nr) {
    Z <- matrix(0, nrow=nc, ncol=nr-nc)
    S <- cbind(diag(sqrt(diag(L2))), Z)
  } else if (nc < nr) {
    Z <- matrix(0, nrow=nr-nc, ncol=nc)
    S <- diag(sqrt(diag(L1)[1:nc]))
    S <- rbind(S, Z)
  }

  adjust <- round((A %*% V) / (U %*% S))[1,]
  V <- V %*% diag(adjust)

  return(list(U=U, S=S, V=V))
}

GolubKahanBidiagonalize <- function(A, QR, reps=100) {
  m <- nrow(A)
  n <- ncol(A)

  X <- A
  U <- diag(m)
  V <- diag(n)

  for (k in 1:n) {
    x <- X[k:m,k]
    e <- rep(0, length(x))
    e[1] <- norm(x, "2") * sign(x[1])
    u <- x + e
    u <- u / norm(u, "2")
    X[k:m, k:n] <- X[k:m, k:n] - 2*u %*% (t(u) %*% X[k:m, k:n])

    if (k <= n-2) {
      x <- X[k,(k+1):n]
      e <- rep(0, length(x))
      e[1] <- norm(x, "2") * sign(x[1])
      v <- x + e
      v <- v / norm(v, "2")
      X[k:m,(k+1):n] <- X[k:m,(k+1):n] - 2*X[k:m,(k+1):n] %*% v %*% t(v)
    }
  }

  return(list(U=t(U),B=X,V=V))
}

SVDfromQRv2 <- function(A, QR, GKB, reps=100) {
  m <- nrow(A)
  n <- ncol(A)

  res <- QR(A)
  Q <- res$Q
  R <- res$R

  B <- GKB(A)

   # <- diag(1, dim(L)[1]);
  for(i in 1:reps) {
    res <- QR(L)
    Q <- res$Q
    Y <- Y %*% Q
    L <- res$R %*% Q
  }

  return(list(U=U, S=S, V=V))
}
```

```{r}
m <- nrow(A)
n <- ncol(A)

res <- SVDfromQR(A, HouseholderQR, 500)
S <- res$S
V <- res$V
U <- res$U

# Compare the matrices with the built-in `svd` function:
A.svd <- svd(A)

U[,1:n] # subset U because the `svd` function doesn't return a square U
U_adj <- U[,1:n] %*% diag(round(A.svd$u/U[,1:n])[1,]) # adjust signs on eigenvectors
all.equal.numeric(A.svd$u, U_adj)

V
V_adj <- V %*% diag(round(A.svd$v/V)[1,]) # adjust signs on eigenvectors
all.equal.numeric(A.svd$v, V_adj)

A.svd$d
diag(S)

# Check the U S V^T returns A
all.equal.numeric(U %*% S %*% t(V), A)
```


## Problem 2
*Calculate the principal components of a covariance matrix $C_{p\times p} (=
\frac{1}{n}A^T A)$ using:*

*(You can use any dataset of $A_{n\times p}$ with $p \geq 10$.)*

```{r}
# n <- 20
# p <- 16
n <- 8
p <- 6
A <- matrix(rnorm(n*p), nrow=n, ncol=p)

pca <- prcomp(A)
```


### i) SVD

The SVD exists for any matrix, so the SVD of matrix $A$ can be used to compute
the eigenvector and eigenvalues of the covariance matrix $X$:

\begin{align}
X &= \frac{1}{n-1} A^T A \\
  &= \frac{1}{n-1} V \Sigma^T U^T U \Sigma V^T \\
  &= \frac{1}{n-1} V \Sigma^2 V^T
\end{align}

The eigenvalues of $X$ are therefore the square of the diagonal of $X$, and the
eigenvector matrix is $V$.

```{r}
n <- ncol(A) # columns are individuals
m <- nrow(A) # rows are features

A_center <- t(apply(A, 1, function(X) { X - colMeans(A) }))

A.svd <- svd(A_center / sqrt(m-1))
S <- diag(A.svd$d)
U <- A.svd$u
V <- A.svd$v

# compare singular values with PCA loadings
diag(S)
all.equal.numeric(diag(S), pca$sdev)

# compare orthonormal bases
V
all.equal.numeric(matrix(pca$rotation, ncol=n), V)
```


### ii) Eigenvalue decomposition

Because the matrix $A$ is not necessarily square, an EVD does not exist. It
must instead be computed on the covariance matrix. This, in effect, means
directly computing the PCA.

```{r}
n <- ncol(A) # columns are individuals
m <- nrow(A) # rows are features

A_center <- t(apply(A, 1, function(X) { X - colMeans(A) }))
X <- 1/(m-1) * t(A_center) %*% A_center

res <- eigen(X)
V <- res$vectors
L <- diag(res$values)

V
V_adj <- V %*% diag(round(pca$rotation/V)[1,]) # adjust signs on eigenvectors
all.equal.numeric(matrix(pca$rotation, ncol=n), V_adj)

diag(L)
all.equal.numeric(pca$sdev^2, diag(L))
```

In this case, some of the vectors from the EVD is the negative of those
obtained by the computing the principal components. This is understandable
given that the vectors correspond to the square of the singular values of $A$.


## Problem 3
*Compare three penalized regression methods: ridge regression, lasso and
elastic net using R package `glmnet` (correspond to $\alpha = 0, 0.5, 1$ in the
objective function). Optimal $\lambda$ can be determined using the cross
validation functionality.*

\begin{equation*}
  \hat\beta = \argmax_\beta \frac{1}{2n} \norm{y - X \beta}_2^2 + \lambda \big( \frac{1}{2}(1-\alpha)\norm{\beta}_2^2 + \alpha \norm{\beta}_1 \big)
\end{equation*}

*For each of the following cases, quantify the MSE and explain why a certain
estimator is better. Generate your data under the true model:*

\begin{equation*}
y = X\beta + \epsilon\textrm{, where } y \in \mathbb{R}^n, X \in \mathbb{R}^{n \times p}\textrm{, } \beta \in \mathbb{R}^p \textrm{, } \epsilon \sim N(0,I_n) \textrm{ and: }
\end{equation*}

### a)
*$p = 5000$, $n=1000$, the first $15$ entries of $\beta$ equal to $1$ and the
other $4085$ equal to $0$, $X_i \sim N(0, I_p)$, $i = 1, \dots, n$.*

```{r, eval=F}
p <- 5000
n <- 1000
b <- c(rep(1,15), rep(0,4985))

mu <- rep(0, p)
S <- diag(p)

X <- rmvnorm(n, mean=mu, sigma=S, method="chol")
y <- X %*% b
```

**Ridge Regression:**

```{r, eval=F}
ridge.fit <- glmnet(X, y, alpha=0)
plot(ridge.fit)

ridge.err <- norm(y - X %*% ridge.fit$beta, "2")
ridge.err

# Cross-validation of lambda values
ridge.cv <- cv.glmnet(X, y, alpha=0)
plot(ridge.cv)
```

**Lasso:**

```{r, eval=F}
lasso.fit <- glmnet(X, y, alpha=1)
plot(lasso.fit)

lasso.err <- norm(y - X %*% lasso.fit$beta, "2")
lasso.err

# Cross-validation of lambda values
lasso.cv <- cv.glmnet(X, y, alpha=1)
plot(lasso.cv)
```

**Elastic Net:**

```{r, eval=F}
elastic.fit <- glmnet(X, y, alpha=0.5)
plot(elastic.fit)

elastic.err <- norm(y - X %*% elastic.fit$beta, "2")
elastic.err

# Cross-validation of lambda values
elastic.cv <- cv.glmnet(X, y, alpha=0.5)
plot(elastic.cv)
```


### b)
*$p = 5000$, $n=1000$, the first $1000$ entries of $\beta$ equal to $1$ and the
other $4000$ equal to $0$, $X_i \sim N(0, I_p)$, $i = 1, \dots, n$.*

```{r, eval=F}
p <- 5000
n <- 1000
b <- c(rep(1,1000), rep(0,4000))

mu <- rep(0, p)
S <- diag(rep(1,p))

X <- rmvnorm(n, mean=mu, sigma=S, method="chol")
y <- X %*% b
```

**Ridge Regression:**

```{r, eval=F}
ridge.fit <- glmnet(X, y, alpha=0)
plot(ridge.fit)

ridge.err <- norm(y - X %*% ridge.fit$beta, "2")
ridge.err

# Cross-validation of lambda values
ridge.cv <- cv.glmnet(X, y, alpha=0)
plot(ridge.cv)
```

**Lasso:**

```{r, eval=F}
lasso.fit <- glmnet(X, y, alpha=1)
plot(lasso.fit)

lasso.err <- norm(y - X %*% lasso.fit$beta, "2")
lasso.err

# Cross-validation of lambda values
lasso.cv <- cv.glmnet(X, y, alpha=1)
plot(lasso.cv)
```

**Elastic Net:**

```{r, eval=F}
elastic.fit <- glmnet(X, y, alpha=0.5)
plot(elastic.fit)

elastic.err <- norm(y - X %*% elastic.fit$beta, "2")
elastic.err

# Cross-validation of lambda values
elastic.cv <- cv.glmnet(X, y, alpha=0.5)
plot(elastic.cv)
```


### c)
*$p = 50$, $n=100$, five entries of $\beta$ equal to $10$, another five equal
to $5$, and the rest $40$ equal to $0$. Each $X_i \sim N(0, \Sigma)$ where
$\Sigma_{jk} = .6^{|i-j|}$ for $j,k = 1, \dots, p$ and $i = 1, \dots, n$.*

```{r, eval=F}
p <- 50
n <- 100
b <- c(rep(10,5), rep(5,5), rep(0, 40))

mu <- rep(0, p)
S <- toeplitz(rep(.6, p)^seq(0, p-1))

X <- rmvnorm(n, mean=mu, sigma=S, method="chol")
y <- X %*% b
```

**Ridge Regression:**

```{r, eval=F}
ridge.fit <- glmnet(X, y, alpha=0)
plot(ridge.fit)

ridge.err <- norm(y - X %*% ridge.fit$beta, "2")
ridge.err

# Cross-validation of lambda values
ridge.cv <- cv.glmnet(X, y, alpha=0)
plot(ridge.cv)
```

**Lasso:**

```{r, eval=F}
lasso.fit <- glmnet(X, y, alpha=1)
plot(lasso.fit)

lasso.err <- norm(y - X %*% lasso.fit$beta, "2")
lasso.err

# Cross-validation of lambda values
lasso.cv <- cv.glmnet(X, y, alpha=1)
plot(lasso.cv)
```

**Elastic Net:**

```{r, eval=F}
elastic.fit <- glmnet(X, y, alpha=0.5)
plot(elastic.fit)

elastic.err <- norm(y - X %*% elastic.fit$beta, "2")
elastic.err

# Cross-validation of lambda values
elastic.cv <- cv.glmnet(X, y, alpha=0.5)
plot(elastic.cv)
```


# Part II: Variant Calling

## Problem 4
*We are calling genotype of an individual from sequencing data. Consider one
position: there are 4 reads mapped to the allele C, the reference allele, and 2
reads mapped to the allele T. The error rate $\epsilon$ is assumed to be 0.03
for all reads.*

### a)
*Use the individual prior defined in slide 16 and genotype likelihood based on
the given error rate, derive the posterior probabilities for genotypes CC, CT
and TT.*

**Genotype: CC**

\begin{align}
P(CC|D) &\propto P(D|CC)P(CC) \\
P(D|CC) &= {(1-\epsilon)}^4 \frac{\epsilon}{3}^2 \\
        &= .97^4 .01^2 = 8.852928\textrm{e-}05 \\
P(CC) &= 1-.001 = .999 \\
\implies P(CC|D) &\propto 8.852928\textrm{e-}05 * .999 = 8.844075\textrm{e-}05
\end{align}


**Genotype: CT**

\begin{align}
P(TC|D) &\propto P(D|TC)P(TC) \\
P(T|G=CT) &= \frac{1}{2}\big(P(T|C) + P(T|T)\big) \\
          &= \frac{1}{2}\big(\frac{\epsilon}{3} + 1-\epsilon\big) \\
          &= \frac{1}{2}\big(1 - \frac{2\epsilon}{3}\big) \\
          &= \frac{1}{2} - \frac{\epsilon}{3} \\
          &= .49 \\
P(C|G=CT) &= \frac{1}{2}\big(P(C|C) + P(C|T)\big) = .49 \\
P(D|CT) &= {.49}^6 = 0.01384129 \\
P(TC) &= .001 * \frac{2}{3} = .000\bar{6} \\
\implies P(CT|D) &\propto 0.01384129 * 0.0006666667 = 9.227527\textrm{e-}06
\end{align}


**Genotype: TT**

\begin{align}
P(TT|D) &\propto P(D|TT)P(TT) \\
P(D|TT) &= {(1-\epsilon)}^2 {\frac{\epsilon}{3}}^4 \\
        &= .97^2 .01^4 = 9.409\textrm{e-}09 \\
P(TT) &= .001 * \frac{1}{3} = .000\bar{3} \\
\implies P(CC|D) &\propto 9.409\textrm{e-}09 * 0.0003333333 = 3.136333\textrm{e-}12
\end{align}

Going by the posteriors, the most likely gene is the homozygote CC.


### b)
*When there is also sequencing information from the individual's family
members, we can use a 'family prior' instead. Specifically, suppose we have
determined the genotypes of the parents as CC and CT. What should be the prior
probabilities of the genotypes given this information? Determine the posterior
probabilities of the child genotypes. Which genotype is the most probable now?*


**New Priors:**

\begin{align}
P(CC|CC,CT) = .5 \\
P(CT|CC,CT) = .5 \\
P(TT|CC,TT) = 0
\end{align}

**Genotype: CC**

\begin{align}
P(CC|D) &\propto 8.852928\textrm{e-}05 * .5 = 4.426464\textrm{e-}05
\end{align}

**Genotype: CT**

\begin{align}
P(CT|D) &\propto 0.01384129 * .5 = 0.006920644
\end{align}


**Genotype: TT**

\begin{align}
P(CC|D) &\propto 9.409\textrm{e-}09 * 0 = 0
\end{align}

With the family prior, the genotype with the highest posterior is CT.


## Problem 5
*Derive the EM algorithm update rule for the problem of estimating genotype
frequencies from sequencing reads (slide 23).*

I will use the notation from the Li paper because that from the slides doesn't
make any fucking sense.

The likelihood for the distribution is:

\begin{align}
L(\xi) &= \prod_{i=1}^n \sum_{g=0}^m L_i(g) \xi_g \\
\implies \ell(\xi) &= \sum_{i=1}^n \log\big(\sum_{g=0}^m L_i(g) \xi_g\big)
\end{align}

To make the problem more tractable, consider the likelihood for a specific
genotype $g=k$. The expression then becomes:

\begin{align}
L(\xi) &= \prod_{i=1}^n \big(L_i(g) \xi_g\big)^{\mathbb{1}\{g=g=k\}} \\
\implies \ell(\xi) &= \sum_{i=1}^n \log \left.\big(L_i(g) \xi_{g}\big)\right|_{g=k} \\
  &= \sum_{i=1}^n \log(\left.L_i(g)\right|_{g=k} ) + \log(\left.\xi_{g}\right|_{g=k})
\end{align}

The update step for the EM algorithm is defined as:

\begin{align}
\xi_g^{(t+1)} &= \argmax_{\xi_g} Q(\theta|\theta^{(t+1)})\textrm{, where} \\
Q(\theta|\theta^{(t+1)}) &= \mathbb{E}_{G|D,\xi^{(t)}} \log P(D,G|\xi) \\
  &= \mathbb{E}_{G|D,\xi^{(t)}} \sum_{i=1}^n \log(\left.L_i(g)\right|_{g=k} ) + \log(\left.\xi_{g}\right|_{g=k}) \\
  &= \gamma_g(k) \sum_{i=1}^n \log(\left.L_i(g)\right|_{g=k} ) + \log(\left.\xi_{g}\right|_{g=k})
\end{align}

The equation is the expectation over the product of the posterior over the
complete data where genotypes are 'known' and the log-likelihood. $D$ is the
data; $G$ are the genotypes; $\gamma_g$ is the posterior probability given the
'known' parameterization; and $\xi$ are the genotypes.

The posterior probability is defined as:

\begin{align}
\gamma_g(k) &= \frac{P(D, \xi^{(t)}|g_i = k)P(g_i=k)}{P(D,\xi^{(t)})} \\
  &= \frac{L_i(k)\xi_k^{(t)}}{\sum_{g=0}^m L_i(g) \xi_g^{(t)}}
\end{align}

To maximize $Q$ given the constraint that the genotype frequencies sum to $1$
(i.e. $\sum_{g=0}^m \xi_g = 1$), use the likelihood and a Lagrange multiplier:

\begin{align}
L(\theta|\theta^{(t+1)}) &= Q(\theta|\theta^{(t+1)}) - \lambda\big(\sum_{g}^m \xi_g - 1\big) \\
  &= \sum_{i=1}^n \gamma_g(k) \big( \log(\left.L_i(g)\right|_{g=k} ) + \log(\left.\xi_{g}\right|_{g=k})\big) - \lambda\big(\sum_{g}^m \xi_g - 1\big)
\end{align}

Take the derivatives to find the maximum:

\begin{align}
\frac{\partial L(\theta|\theta^{(t+1)})}{\partial \xi_k} &= \sum_{i=1}^n \gamma_g(k) \frac{1}{\xi_{k}} - \lambda = 0 \\
\implies \frac{1}{\lambda} \sum_{i=1}^n \gamma_g(k) &= {\xi_k}
\end{align}

Summing across all $m$ genotypes allows and using the constraint $\sum_{g=0}^m
\xi_g = 1$:

\begin{align}
\sum_{g=0}^m \frac{1}{\lambda} \sum_{i=1}^n \gamma_g(k) &= \sum_{g=0}^m \xi_g = 1 \\
\implies \sum_{g=0}^m \sum_{i=1}^n \gamma_g(k) &= \lambda \\
\implies \sum_{i=1}^n \sum_{g=0}^m \gamma_g(k) &= \lambda \\
\implies \sum_{i=1}^n 1 &= \lambda \\
\implies n &= \lambda
\end{align}

Therefore, the update step is:

\begin{align}
\xi_k^{(t+1)} &= \frac{1}{n} \sum_{i=1}^n \gamma_g(k) \\
  &= \frac{1}{n} \sum_{i=1}^n \frac{L_i(k)\xi_k^{(t)}}{\sum_{g=0}^m L_i(g) \xi_g^{(t)}}
\end{align}

Using Bayes rule, this equation can be further simplified:

\begin{align}
\xi_k^{(t+1)} &= \frac{1}{n} \sum_{i=1}^n \frac{L_i(k)\xi_k^{(t)}}{\sum_{g=0}^m L_i(g) \xi_g^{(t)}} \\
  &= \frac{1}{n} \sum_{i=1}^n \frac{P(d_i|\xi^{(t)}, g)P(g)}{\sum_{g=0}^m P(d_i|\xi^{(t)}, g)P(g)} \\
  &= \frac{1}{n} \sum_{i=1}^n \frac{P(d_i|\xi^{(t)}, g)P(g)}{P(d_i|\xi^{(t)})} \\
  &= \frac{1}{n} \sum_{i=1}^n \frac{P(g|d_i,\xi^{(t)})P(d_i|\xi^{(t)})}{P(d_i|\xi^{(t)})} \\
  &= \frac{1}{n} \sum_{i=1}^n P(g|d_i,\xi^{(t)})
\end{align}

Note that $d_i$ are the reads from individual $i$, and $g$ is the genotype such
that $g \in G$, where $G$ is the set of all possible genotypes and where $|G| =
m$.


## Problem 6
*We are performing an association study using whole-exome or genome sequencing.
The basic analysis is for each variant, detect if its allele frequencies
(assume Hardy-Weinberg equilibrium) are different between cases and controls.*

### a)
*Suppose cases and controls are sequenced at different labs, with possibly
different sequencing depth. If we call genotypes of each individual first, and
then compare the frequency of alleles between cases and controls to detect
association, what would be the problem of this procedure?*

Any differences between cases and controls could be an artifact from the
sequencing procedure.


### b)
*Describe how you would address this problem.*

One can compensate for bias by using a linear model that relates the sequence
calls from a particular sequencing platform and read depth to the known
sequence posterior. (The posterior distribution for sequence calls is generated
by a HMM for the sequence, multiplied over the number of reads.) Regress an
estimate of the posterior probabilities of nucleotide calls against the known
distribution at each locus to obtain a set of locus-specific correction
factors, which can then be used as the coefficients to adjust likelihoods for
unknown data.

This strategy will require a regression to be performed at every locus, because
the bias for a given sequence in the genome depends on many sequence specific
factors, such as GC content, repeats, etc. This means that a regression model
for a given sequence is likely unusable for a sequence from a different part of
the genome. To generalize the linear model, one could pick out features that
correspond to high regression coefficients (i.e. high deviation from the
expected posterior).  This could be done manually based on known molecular
features of DNA or the sequencing technology, or it could be done in an
automated manner by a machine learning algorithm.
