---
title: Homework 8
author: Ansel George
output:
  pdf_document:
    latex_engine: xelatex
    highlight: tango
fontsize: 11pt
mainfont: Noto Sans
monofont: Ubuntu Mono
---

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\argmin}{\operatornamewithlimits{arg\ min}}
\newcommand{\argmax}{\operatornamewithlimits{arg\ max}}

```{r, message=F}
library(glmnet)
library(mvtnorm)

set.seed(10)
```

# Part I: PCA & Penalized Regression

## Problem 1

### a)
*Explain the relationship between SVD, PCA, QR and eigenvalue decomposition.*

SVD: Singular value decomposition differs from PCA in that it exists for any
matrix, including non-square ones.

QR factorization exists for any matrix and decomposes a matrix $A$ into the
product $A=QR$, where $Q$ is an orthonormal matrix and $R$ is upper triangular.

An EVD, which only exists for invertible matrices, decomposes a matrix $A$ into
the products $A=V\Lambda V^T$, where $V$ is the set of eigenvectors and
$\Lambda$ a diagonal set of eigenvalues. If one were to multiply $\Lambda V^T$,
it would result in an upper-triangular matrix - $R$ in the formula for the $QR$
factorization. Also, because $V$ is a set of eigenvectors, they are by
definition orthogonal - $Q$ in terms of the $QR$ factorization.


### b)
*Implement Gram-Schmidt orthogonalization, and use it to perform QR, SVD and
PCA.*

Gram-Schmidt is poorly conditioned and in general is a stupid idea, so I'll use
Householder reflections instead. They work by sequential rotations of the
original matrix, an operation where the condition number is 1. The result is
that errors in $A$ aren't amplified over sequential operations, assuming $A$
represents real data that contains noise, etc. from measurement.

```{r}
qr_householder <- function(X) {
  nr <- nrow(X)
  nc <- ncol(X)

  Q <- diag(1, nr)
  R <- X

  for(i in 1:(nc-1)) {
    v <- R[i:nr, i]
    e <- rep(0, length(v))
    e[1] <- norm(v, "2") * sign(-X[i, i])
    v <- v + e
    v <- v / norm(v, "2")
    H <- diag(nr)
    H[i:nr, i:nr] <- H[i:nr, i:nr] - 2*outer(v, v)
    R <- H %*% R
    Q <- Q %*% H
  }

  return(list(Q=Q,R=R))
}
```

```{r}
n <- 10
p <- 8
A <- matrix(rnorm(n*p), nrow=n, ncol=p)
```

**QR:**

```{r}
res <- qr_householder(X)
res$Q
res$R
```

**QR and PCA:**

```{r}
X <- 1/n * A %*% t(A) # get covariance matrix for PCA
reps <- 1000

V <- diag(1, dim(D)[1]);
for(i in 1:reps) {
  res <- qr_householder(X)
  Q <- res$Q
  V <- V %*% Q
  X <- res$R %*% Q
}
```

```{r}
# Compare values with the built-in `eigen` function:
res <- eigen(X)

res$values
diag(X)

res$vectors
V
```

**QR and SVD:**


## Problem 2
*Calculate the principal components of a covariance matrix $C_{p\times p} (=
\frac{1}{n}A^T A)$ using:*

*(You can use any dataset of $A_{n\times p}$ with $p \geq 10$.)*

```{r}
n <- 10
p <- 8
A <- matrix(rnorm(n*p), nrow=n, ncol=p)
X <- 1/n * A %*% t(A)
```

### i) SVD

The SVD exists for any matrix, so the SVD of matrix $A$ can be used to compute
the eigenvector and eigenvalues of the covariance matrix:



### ii) Eigenvalue decomposition


## Problem 3
*Compare three penalized regression methods: ridge regression, lasso and
elastic net using R package `glmnet` (correspond to $\alpha = 0, 0.5, 1$ in the
objective function). Optimal $\lambda$ can be determined using the cross
validation functionality.*

\begin{equation*}
\hat\beta = \argmax_\beta \frac{1}{2n} \norm{y - X \beta}_2^2 + \lambda \big( \frac{1}{2}(1-\alpha)\norm{\beta}_2^2 + \alpha \norm{\beta}_1 \big)
\end{equation*}

*For each of the following cases, quantify the MSE and explain why a certain
estimator is better. Generate your data under the true model:*

\begin{equation*}
y = X\beta + \epsilon\textrm{, where } y \in \mathbb{R}^n, X \in \mathbb{R}^{n \times p}\textrm{, } \beta \in \mathbb{R}^p \textrm{, } \epsilon \sim N(0,I_n) \textrm{ and: }
\end{equation*}

### a)
*$p = 5000$, $n=1000$, the first $15$ entries of $\beta$ equal to $1$ and the
other $4085$ equal to $0$, $X_i \sim N(0, I_p)$, $i = 1, \dots, n$.*

```{r}
p <- 5000
n <- 1000
b <- c(rep(1,15), rep(0,4985))

mu <- rep(0, p)
S <- diag(rep(1,p))

X <- rmvnorm(n, mean=mu, sigma=S, method="chol")
```

**Ridge Regression:**

```{r}
rrfit <- glmnet(X, X %*% b, alpha=0)
plot(rrfit$lambda)
lines(rrfit$lambda)
```



**LASSO:**

**Elastic Net:**


### b)
*$p = 5000$, $n=1000$, the first $1000$ entries of $\beta$ equal to $1$ and the
other $4000$ equal to $0$, $X_i \sim N(0, I_p)$, $i = 1, \dots, n$.*

```{r}
p <- 5000
n <- 1000
b <- c(rep(1,1000), rep(0,4000))

mu <- rep(0, p)
S <- diag(rep(1,p))

X <- rmvnorm(n, mean=mu, sigma=S, method="chol")
```

### c)
*$p = 50$, $n=100$, five entries of $\beta$ equal to $10$, another five equal
to $5$, and the rest $40$ equal to $0$. Each $X_i \sim N(0, \Sigma)$ where*
$\Sigma_{jk} = .6^{|i-j|}$ for $j,k = 1, \dots, p$ and $i = 1, \dots, n$.

```{r}
p <- 50
n <- 100
b <- c(rep(10,5), rep(5,5), rep(0, 40))

mu <- rep(0, p)


X <- rmvnorm(n, mean=mu, sigma=S, method="chol")
```


# Part II: Variant Calling

## Problem 4
*We are calling genotype of an individual from sequencing data. Consider one
position: there are 4 reads mapped to the allele C, the reference allele, and 2
reads mapped to the allele T. The error rate ε is assumed to be 0.03 for all
reads.*

### a)
*Use the individual prior defined in slide 16 and genotype likelihood based on
the given error rate, derive the posterior probabilities for genotypes CC, CT
and TT.*

### b)
*When there is also sequencing information from the individual’s family
members, we can use a ‘family prior’ instead. Specifically, suppose we have
determined the genotypes of the parents as CC and CT. What should be the prior
probabilities of the genotypes given this information? Determine the posterior
probabilities of the child genotypes. Which genotype is the most probable now?*


## Problem 5
*Derive the EM algorithm update rule for the problem of estimating genotype
frequencies from sequencing reads (slide 23).*

## Problem 6
*We are performing an association study using whole-exome or genome sequencing.
The basic analysis is for each variant, detect if its allele frequencies
(assume Hardy-Weinberg equilibrium) are different between cases and controls.*

### a)
*Suppose cases and controls are sequenced at different labs, with possibly
different sequencing depth. If we call genotypes of each individual first, and
then compare the frequency of alleles between cases and controls to detect
association, what would be the problem of this procedure?*

Any differences between cases and controls could be an artifact from the
sequencing procedure.


### b)
*Describe how you would address this problem.*
