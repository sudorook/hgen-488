---
title: Homework 8
author: Ansel George
output:
  pdf_document:
    latex_engine: xelatex
    highlight: tango
fontsize: 11pt
mainfont: Noto Sans
monofont: Ubuntu Mono
---

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\argmin}{\operatornamewithlimits{arg\ min}}
\newcommand{\argmax}{\operatornamewithlimits{arg\ max}}

```{r, message=F}
library(glmnet)
library(mvtnorm)

set.seed(10)
```

# Part I: PCA & Penalized Regression

## Problem 1

### a)
*Explain the relationship between SVD, PCA, QR and eigenvalue decomposition.*

SVD: Singular value decomposition differs from PCA in that it exists for any
matrix, including non-square ones.

QR factorization exists for any matrix and decomposes a matrix $A$ into the
product $A=QR$, where $Q$ is an orthonormal matrix and $R$ is upper triangular.

An EVD, which only exists for invertible matrices, decomposes a matrix $A$ into
the products $A=V\Lambda V^T$, where $V$ is the set of eigenvectors and
$\Lambda$ a diagonal set of eigenvalues. If one were to multiply $\Lambda V^T$,
it would result in an upper-triangular matrix - $R$ in the formula for the $QR$
factorization. Also, because $V$ is a set of eigenvectors, they are by
definition orthogonal - $Q$ in terms of the $QR$ factorization.


### b)
*Implement Gram-Schmidt orthogonalization, and use it to perform QR, SVD and
PCA.*

Gram-Schmidt is poorly conditioned and in general is a stupid idea, especially
when iteratively computing QR factorizations to get the SVD and PCA, so I'll
use Householder reflections instead. They work by sequential reflections of the
original matrix --- an operation where the condition number is 1. The result is
that errors in $A$ aren't amplified over sequential operations, assuming $A$
represents real data that contains noise, etc. from measurement.

```{r}
HouseholderQR <- function(X) {
  m <- nrow(X)
  n <- ncol(X)

  if (m > n) {
    l <- n
  } else {
    l <- m
  }

  Q <- diag(m)
  R <- X

  for(i in 1:(l)) {
    v <- R[i:m, i]
    e <- rep(0, length(v))
    e[1] <- norm(v, "2") * sign(v[1])
    v <- v + e
    # if ((length(v) == 1) && (all.equal(v, 0) == T)) {
    #   v <- 0
    # } else {
    #   v <- v / norm(v, "2")
    # }
    v <- v / norm(v, "2")
    H <- diag(m)
    H[i:m, i:m] <- H[i:m, i:m] - 2*outer(v, v)
    R <- H %*% R
    Q <- Q %*% H
  }

  return(list(Q=Q,R=R))
}

GramSchmidtQR <- function(A) {
  m <- dim(A)[1]
  n <- dim(A)[2]

  Q <- matrix(0, nrow=m, ncol=n)
  R <- matrix(0, nrow=n, ncol=n)

  for (j in 1:n) {
    v <- A[,j]
    for (i in 1:j) {
      R[i,j] <- Q[,i] %*% A[,j]
      v <- v - R[i,j] * Q[,i]
    }
    R[j,j] <- norm(v, type="2")
    Q[,j] <- v / R[j,j]
  }

  return(list(Q=Q, R=R))
}
```

```{r}
n <- 10
p <- 8
A <- matrix(rnorm(n*p), nrow=n, ncol=p)
```

**QR:**

```{r}
res <- HouseholderQR(A)
Q <- res$Q
R <- res$R
```

```{r}
round(Q, 8) # round to make 0's more obvious
round(R, 8) # round to make 0's more obvious

all.equal(Q %*% t(Q), diag(dim(Q)[1])) # check that Q is orthogonal
all.equal(Q %*% R, A)
```


**QR and PCA:**

The PCA exists for the symmetric matrix $X$ where the mean of $X$ is centered
at 0, and the eigenvalues and eigenvectors can be found though the iterative
algorithm:

\begin{equation}
  \begin{split}
    X^{(t)} = Q^{(t)} R^{(t)} \\
    X^{(t+1)} = R^{(t)} Q^{(t)}
  \end{split}
\begin{equation}

The PCA of X takes the form:

\begin{equation}
  X = V \Lambda V^T
\end{equation}

where $V$ is the set of eigenvectors and $\Lambda$ the diagonal of
corresponding eigenvalues.

The iteration has the property that:

\begin{align}
  X^{(t+1)} &= R^{(t)} Q^{(t)} \\
            &= {Q^{(t)}}^T Q^{(t)} R^{(t)} Q^{(t)} \\
            &= {Q^{(t)}}^T X^{(t)} Q^{(t)} \\
  X^{(t+2)} &= R^{(t+1)} Q^{(t+1)} \\
            &= {Q^{(t+1)}}^T Q^{(t+1)} R^{(t+1)} Q^{(t+1)} \\
            &= {Q^{(t+1)}}^T X^{(t+1)} Q^{(t+1)} \\
            &= {Q^{(t+1)}}^T {Q^{(t)}}^T X^{(t)} Q^{(t)} Q^{(t+1)} \\
  \vdots \\
  X^{(n)} &= {\prod^n Q^{(i)}}^T X^{(t=1)} \prod^n Q^{(i)}
\end{align}


```{r}
PCAfromQR <- function(A, QR, reps=100) {
  m <- dim(A)[1]
  n <- dim(A)[2]

  # Center the data and compute the covariance matrix
  A_center <- A - apply(A, 2, mean)
  A <- apply(A, 1, function(x) {x - colMeans(A)})
  A <- t(A)
  X <- cov(A)

  V <- diag(1, dim(X)[1]);
  L <- X

  for(i in 1:reps) {
    res <- QR(L)
    Q <- res$Q
    R <- res$R
    V <- V %*% Q
    L <- R %*% Q
  }

  return(list(V=V, L=L))
}
```

```{r}
res <- PCAfromQR(A, HouseholderQR, reps=100)
V <- res$V
L <- res$L

# Compare values with the built-in `prcomp` function:
res <- prcomp(A)

# Note that A is not full rank, so the last two eigenvectors and
# eigenvectors are nonsense. 
res$sdev^2
diag(L)

res$rotation
V

all.equal(res$rotation, V %*% diag(round(res$rotation/V)[1,]))
```


**QR and SVD:**

```{r}
SVDfromQR <- function(X, QR, reps=100) {
  nc <- ncol(X)
  nr <- nrow(X)

  L1 <- X %*% t(X)
  U <- diag(1, dim(L1)[1]);
  for(i in 1:reps) {
    res <- QR(L1)
    Q <- res$Q
    U <- U %*% Q
    L1 <- res$R %*% Q
  }

  L2 <- t(X) %*% X
  V <- diag(1, dim(L2)[1]);
  for(i in 1:reps) {
    res <- QR(L2)
    Q <- res$Q
    V <- V %*% Q
    L2 <- res$R %*% Q
  }

  # Pad the sigma matrix or subset it based on the dimensions of the input
  # matrix.
  if (nc > nr) {
    Z <- matrix(0, nrow=nc, ncol=nr-nc)
    S <- cbind(diag(sqrt(diag(L2))), Z)
  } else if (nc < nr) {
    Z <- matrix(0, nrow=nr-nc, ncol=nc)
    S <- diag(sqrt(diag(L1)[1:nc]))
    S <- rbind(S, Z)
  }

  adjust <- round((A %*% V) / (U %*% S))[1,]
  V <- V %*% diag(adjust)

  return(list(U=U, S=S, V=V))
}

GolubKahanBidiagonalize <- function(A, QR, reps=100) {
  m <- nrow(A)
  n <- ncol(A)

  X <- A
  U <- diag(m)
  V <- diag(n)

  for (k in 1:n) {
    x <- X[k:m,k]
    e <- rep(0, length(x))
    e[1] <- norm(x, "2") * sign(x[1])
    u <- x + e
    u <- u / norm(u, "2")
    X[k:m, k:n] <- X[k:m, k:n] - 2*u %*% (t(u) %*% X[k:m, k:n])

    if (k <= n-2) {
      x <- X[k,(k+1):n]
      e <- rep(0, length(x))
      e[1] <- norm(x, "2") * sign(x[1])
      v <- x + e
      v <- v / norm(v, "2")
      X[k:m,(k+1):n] <- X[k:m,(k+1):n] - 2*X[k:m,(k+1):n] %*% v %*% t(v)
    }
  }

  return(list(U=t(U),B=X,V=V))
}

SVDfromQRv2 <- function(A, QR, GKB, reps=100) {
  m <- nrow(A)
  n <- ncol(A)

  res <- QR(A)
  Q <- res$Q
  R <- res$R

  B <- GKB(A)

   # <- diag(1, dim(L)[1]);
  for(i in 1:reps) {
    res <- QR(L)
    Q <- res$Q
    Y <- Y %*% Q
    L <- res$R %*% Q
  }

  return(list(U=U, S=S, V=V))
}
```

```{r}
res <- SVDfromQR(A, HouseholderQR, 500)
S <- res$S
V <- res$V
U <- res$U

# Compare the matrices with the built-in `svd` function:
A.svd <- svd(A)

A.svd$u
U[,1:8]

A.svd$v
V

A.svd$d
diag(S)

# Check the U S V^T returns A
all.equal(U %*% S %*% t(V), A)

U %*% S %*% t(V)
A
```


## Problem 2
*Calculate the principal components of a covariance matrix $C_{p\times p} (=
\frac{1}{n}A^T A)$ using:*

*(You can use any dataset of $A_{n\times p}$ with $p \geq 10$.)*

```{r}
n <- 10
p <- 8
A <- matrix(rnorm(n*p), nrow=n, ncol=p)
X <- 1/n * A %*% t(A)
```

### i) SVD

The SVD exists for any matrix, so the SVD of matrix $A$ can be used to compute
the eigenvector and eigenvalues of the covariance matrix $X$:

\begin{align}
X &= \frac{1}{n} A A^T \\
  &= \frac{1}{n} U \Sigma V^T V \Sigma^T U^T \\
  &= \frac{1}{n} U \Sigma^2 U^T
\end{align}

The eigenvalues of $X$ are therefore the square of the diagonal of $X$, and the
eigenvector matrix is $U$.

```{r}
n <- ncol(A) # columns are individuals, rows are features

res <- svd(A)
S <- res$d
U <- res$u

S^2 / n # loadings
U       # component vectors

# Compare with the direct PCA on individuals in feature-space
res <- eigen(X)
res$values
res$vectors
```


### ii) Eigenvalue decomposition

Because the matrix $A$ is not necessarily square, an EVD does not exist.


## Problem 3
*Compare three penalized regression methods: ridge regression, lasso and
elastic net using R package `glmnet` (correspond to $\alpha = 0, 0.5, 1$ in the
objective function). Optimal $\lambda$ can be determined using the cross
validation functionality.*

\begin{equation*}
\hat\beta = \argmax_\beta \frac{1}{2n} \norm{y - X \beta}_2^2 + \lambda \big( \frac{1}{2}(1-\alpha)\norm{\beta}_2^2 + \alpha \norm{\beta}_1 \big)
\end{equation*}

*For each of the following cases, quantify the MSE and explain why a certain
estimator is better. Generate your data under the true model:*

\begin{equation*}
y = X\beta + \epsilon\textrm{, where } y \in \mathbb{R}^n, X \in \mathbb{R}^{n \times p}\textrm{, } \beta \in \mathbb{R}^p \textrm{, } \epsilon \sim N(0,I_n) \textrm{ and: }
\end{equation*}

### a)
*$p = 5000$, $n=1000$, the first $15$ entries of $\beta$ equal to $1$ and the
other $4085$ equal to $0$, $X_i \sim N(0, I_p)$, $i = 1, \dots, n$.*

```{r}
p <- 5000
n <- 1000
b <- c(rep(1,15), rep(0,4985))

mu <- rep(0, p)
S <- diag(rep(1,p))

X <- rmvnorm(n, mean=mu, sigma=S, method="chol")
```

**Ridge Regression:**

```{r}
rrfit <- glmnet(X, X %*% b, alpha=0)
plot(rrfit$lambda)
lines(rrfit$lambda)
```



**LASSO:**

**Elastic Net:**


### b)
*$p = 5000$, $n=1000$, the first $1000$ entries of $\beta$ equal to $1$ and the
other $4000$ equal to $0$, $X_i \sim N(0, I_p)$, $i = 1, \dots, n$.*

```{r}
p <- 5000
n <- 1000
b <- c(rep(1,1000), rep(0,4000))

mu <- rep(0, p)
S <- diag(rep(1,p))

X <- rmvnorm(n, mean=mu, sigma=S, method="chol")
```

### c)
*$p = 50$, $n=100$, five entries of $\beta$ equal to $10$, another five equal
to $5$, and the rest $40$ equal to $0$. Each $X_i \sim N(0, \Sigma)$ where*
$\Sigma_{jk} = .6^{|i-j|}$ for $j,k = 1, \dots, p$ and $i = 1, \dots, n$.

```{r}
p <- 50
n <- 100
b <- c(rep(10,5), rep(5,5), rep(0, 40))

mu <- rep(0, p)


X <- rmvnorm(n, mean=mu, sigma=S, method="chol")
```


# Part II: Variant Calling

## Problem 4
*We are calling genotype of an individual from sequencing data. Consider one
position: there are 4 reads mapped to the allele C, the reference allele, and 2
reads mapped to the allele T. The error rate ε is assumed to be 0.03 for all
reads.*

### a)
*Use the individual prior defined in slide 16 and genotype likelihood based on
the given error rate, derive the posterior probabilities for genotypes CC, CT
and TT.*

### b)
*When there is also sequencing information from the individual’s family
members, we can use a ‘family prior’ instead. Specifically, suppose we have
determined the genotypes of the parents as CC and CT. What should be the prior
probabilities of the genotypes given this information? Determine the posterior
probabilities of the child genotypes. Which genotype is the most probable now?*


## Problem 5
*Derive the EM algorithm update rule for the problem of estimating genotype
frequencies from sequencing reads (slide 23).*

## Problem 6
*We are performing an association study using whole-exome or genome sequencing.
The basic analysis is for each variant, detect if its allele frequencies
(assume Hardy-Weinberg equilibrium) are different between cases and controls.*

### a)
*Suppose cases and controls are sequenced at different labs, with possibly
different sequencing depth. If we call genotypes of each individual first, and
then compare the frequency of alleles between cases and controls to detect
association, what would be the problem of this procedure?*

Any differences between cases and controls could be an artifact from the
sequencing procedure.


### b)
*Describe how you would address this problem.*
