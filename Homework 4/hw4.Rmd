---
title: Homework 4
author: Ansel George
output: pdf_document
---

```{r, message=F}
library(stringr)
library(dplyr)
library(tidyr)
library(igraph)

# set.seed(2)
set.seed(8)
```

# Problem 1

*Can you build a hash table for 3-tuples for the following sequences and search
for string "TAGCTAGCT"? Write down the pseudo code and then implement it.*

```
S1 = "GCTGCTGCTGCTAAACGTTTGGGGCAGTCGAT"
S2 = "GGTGCTCCAAGCTTTTGAGTCTGCTAGTGTCAACCCT"
S3 = "GTGGGCCCCCTAGCTAGCTAGCTGGGGCAC"
S4 = "TGTCGCTGGCTGGACTGCTGATCGTAGTAG"
```

## Pseudocode

```
MAKE_GLOBAL_HASH_TABLE(sequences):
  for each sequence in sequences:
    for each triplet in sequence:
      value = hash_function(triplet)
      append (sequence_id, triplet_position) to hash_table[value]
    end
  end
end

MAKE_SEARCH_HASH_TABLE(substring):
  for each triplet in substring:
    value = hash_function(triplet)
    append (sequence_id, substring_position, triplet_position) to substring_table[value]
  end
end

HASH_SEARCH(table, substring):
  substring_table = MAKE_SEARCH_HASH_TABLE(substring)
  subset_global_table = global_hash_table[values in substring_table]
  for each candidate in subset_global_table:
    append continguous set of candidate triplets to results
  end
  return results
end
```


## Implementation

Code copied below.

The implementation uses overlapping triplet windows for generating the hash
table. This will take more space and be more computationally intensive in terms
of creating the table and searching, albeit at the same order complexity, but
it will have the advantage of allowing searches for substrings that do not have
a length that is a multiple of three.


```{bash}
cat hashtable/src/main.cpp
```


## Substring search

To run it yourself, run:

```{bash}
cd hashtable
make clean
make && ./substring_search
```


# Problem 2

*Implement Burrows-Wheeler Transform of "DOGWORD" and obtain its LF mapping.
How can we search for "GWO" using FM index (implementation is optional, just
write down the procedure)?*

Code copies below.

```{bash}
cat bwt/src/main.cpp
```

To run it yourself, run:

```{bash}
cd bwt
make clean
make && ./bwt
```


# Problem 3

*Give an $O(n \log k)$-time algorithm to merge $k$ sorted lists into one sorted
list, where $n$ is the total number of elements in all the input lists. (Hint:
Use a min-heap for $k$-way merging.)*

The following algorithm can be adjusted to sort the merged lists in ascending
or descending order based on one's preference. As written, it will return a
merged list in ascending order.

1. Initialization:
   - Create a heap from the minimum values of the $k$ sorted lists.
   - Push the items from their sorted lists into the heap, so that the length
     of each of the $k$ lists has decreased by 1.
   - Runtime: $O(k \log k)$.

2. Iteration:
   - Take the minimum of the heap of minimum values and push to a list to
     contain the $n$-element sort. $O(1)$
   - Update the heap after removing the root. $O(\log k)$
   - Pop the minimum element from the list the root came from. $O(1)$
      * If the list is empty, skip this step.
   - Push the minimum element to the heap of minimum values and update the keys. $O(\log k)$

3. Termination:
   - Continue popping from the minimum heap and pushing items from the $k$
     sorted lists until all $n$ items have been processed.

The runtime is:

\begin{align}
T(n) &= O( k \log k + n (\log k + \log k) ) \\
  &= O( (2n+k) \log k) \\
  &\sim O(n \log k ) \textrm{ when }n \gg k
\end{align}


# Problem 4

*Implement genome assembly using the De Bruijn graph approach following these
steps:*

## (i) Simulate the genome
*Sample a DNA sequence of length 1000 bp. You could just use uniform
distribution over the four nucleotides ATCG.*

```{r}
dna_length <- 1000
nucleotides <- c("A", "T", "C" ,"G")

# Sample the nucleotides with a uniform distribution.
dna <- sample(nucleotides, dna_length, replace=T)
dna <- paste(unlist(dna), collapse='')
```

## (ii) Simulate read data
*Suppose the read length is 25 bp, sample 400 reads from the genome,
with uniform start position.*

```{r}
read_length <- 25
read_count <- 400

# Generate a random set of starting points for each read, sampled uniformly
# from the range of possible starting points over the DNA sequence.
read_start <- sample(seq(1, dna_length-read_length+1), read_count, replace=T)
reads <- substring(dna, first=read_start, last=read_start+read_length-1)
```

## (iii) Construct a graph of $k$-mers with $k = 10$.

```{r}
# Generate the spectrum for a sequence `string` using k-mers of size `size`.
getSpectrum <- function(string, size) {
  # Fail of the size exceeds the string length.
  if (size > str_length(string)) {
    return(-1)
  }

  # Using a sliding k-mer window over the string, append each string to a set
  # of all k-mer windows.
  spectrumSet <- c()
  for (i in 1:(str_length(string)-size+1)) {
    spectrumSet <- append(spectrumSet, str_sub(string, i, size+i-1))
  }
  return(spectrumSet)
}
```

```{r}
kmer_length <- 10

# Get spectrum of all reads.
spectrumSet <- c()
for (read in reads) {
  spectrumSet <- append(spectrumSet, getSpectrum(read, kmer_length))
}

# Use dplyr for ease of computing the spectrum and handling multiple
# occurrences of a particular read.
spectrum_df <- tbl_df(data.frame("kmer"=spectrumSet))
spectrum_df <- spectrum_df %>% group_by(kmer) %>% count()

# Make the adjacency matrix for the de Bruijn graph.
G <- matrix(0, nrow=nrow(spectrum_df), ncol=nrow(spectrum_df))

for (i in 1:nrow(G)) {
  for (j in 1:ncol(G)) {
    if (j == i) next # don't allow self-edges
    i1 <- str_sub(spectrum_df$kmer[i], 1, kmer_length-1)
    j1 <- str_sub(spectrum_df$kmer[j], 2, kmer_length)
    if (j1 == i1) {
      G[j,i] <- G[j,i] + spectrum_df$n[j]
    }
  }
}
```


## (iv) Find the Eulerian path of the graph.
*Ideally, you solve this by writing your own code. It is acceptable if you use
some package with graph functions to solve this.*

```{r}
# Function to generate the sequence from the path through the De Bruijn graph.
getFullSequence <- function(paths, subsequences) {
  full_sequence <- subsequences[paths[1]]
  for (i in 2:length(paths)) {
    full_sequence <- str_c(full_sequence, str_sub(subsequences[paths[i]], -1))
  }
  return(full_sequence)
}

# Compute the Eulerian path through De Bruijn graph `G`, where the indices of
# the graph correspond to sequence data for each k-mer stored in
# `subsequences`.
# If there is no path through the graph (# unbalanced nodes > 2), the function
# will return with an error.
getEulerPath <- function(G, subsequences) {
  M <- 1*(G>0) # matrix of edges, ignore the weights

  # Find the number of unbalanced nodes (colsums != rowsums)
  unbalanced_nodes <- which(colSums(M)-rowSums(M) != 0, arr.ind=T)
  unbalanced_scores <- (colSums(M)-rowSums(M))[unbalanced_nodes]
  source_nodes <- which(colSums(M) == 0) # nodes with no incoming edges
  sink_nodes <- which(rowSums(M) == 0) # nodes with no outgoing edges

  if ( (length(source_nodes) != length(sink_nodes)) ||
      (length(unbalanced_nodes) > 2) ) {
    print("This is bad. No Eulerian path exists.")
    return(-1)
  }

  # If every node is perfectly balanced, just start anywhere. If not, start at
  # the source node.
  if (length(source_nodes) == 0) {
    idx <- 1
  } else {
    idx <- source_nodes[1]
  }
  paths <- c(idx)
  subpath <- c()
  queue <- c()
  origin <- source_nodes[1]
  edgesum <- sum(M)+1

  # Continue iterating until all edges are found. (Or some error occurs and
  # causes premature exiting).
  while ( length(paths) != edgesum ) {
    # The new index is found by finding a non-zero entry in the row of idx.
    new_idx <- which(M[idx,] > 0)

    # If no new index is found, then check that no unvisited edges in the
    # `queue` exist. If yes, then set the idx variable to the first value in
    # the queue and continue.
    if (length(new_idx) == 0) {
      # If the path has ended up where it started, merge any existing subpath.
      if (idx == origin) {
        if (length(subpath) > 0) {
          paths <- append(paths, subpath)
          subpath <- c()
        }
      }
      # If the path has reached a dead-end, check if the queue is empty. If
      # not, then pop the first element of the queue as the new index, split
      # the path into two halved: one leading up to the idx and one with the
      # remainder. The subpath will be appended back to the path when the idx
      # == origin (when the loop loops back on itself).
      if (length(queue) > 0) {
        idx <- queue[1]
        queue <- queue[-1]
        split_idx <- which(paths == idx)
        subpath <- paths[(split_idx+1):length(paths)]
        paths <- paths[1:(split_idx)]
        origin <- idx
        next
      }
      break # if there's nowhere else to go, just quit
    } else if (length(new_idx) > 1) {
      # when multiple outgoing nodes, push to queue
      queue <- c(queue, idx)
      M[idx, new_idx[1]] <- 0
      idx <- new_idx[1]
    } else {
      # keep travelling the path
      M[idx, new_idx] <- 0
      idx <- new_idx
    }

    paths <- c(paths, idx)
  }

  # Generate the subsequence from the path
  subseq <- getFullSequence(paths, subsequences)

  return(list(sequence=subseq, path=paths))
}
```

```{r}
res <- getEulerPath(G, spectrum_df$kmer)
head(res$path)
res$sequence
dna

# Check that sequence produced matches the DNA input
res$sequence == dna
```

```{r}
f<-file("output.txt")
writeLines(c(dna, res$sequence), f)
close(f)
```


## (v) Report the statistics of your result
*Comparing with the true genome sequences. For example, you can do an alignment
of the two sequences and report the percentage of identity.*

The assembled and true sequence are equal (100% identity). This is largely due
to chance, as the random structure of the DNA sequence:

 * Could have redundant kmers that result in multiple possible Eulerian paths.
 * Could have insufficient coverage, leaving gaps in reads and >2 unbalanced
   nodes.

```{r}
# Check that sequence produced matches the DNA input
res$sequence == dna
```


# Problem 5

*We extend PCST algorithm in class to find disease related gene modules:*

*Instead of having an unweighted network, we now have co-expression gene
network. In this undirected network, any two genes are linked with an edge,
with the edge weight being the correlation of gene expression (-1 to 1) across
a given set of gene expression datasets. We assume that genes with high
co-expression (both positive and negative) are likely to be involved in the
same biological processes.*

*Given the disease relevance of genes (p-values) and the weighted co-expression
network,*

## a)
*Extend the PCST algorithm discussed in class to find disease related gene
modules. We have two criteria for choosing gene modules: they should be
enriched with genes with high disease relevance (small p-values), and the genes
in the subgraphs should be tightly co-expressed.*

All nodes have p-values between 0 and 1, and all the correlations are between
-1 and 1. The goal is to find subgraphs with low p-values and high correlation,
which is opposite what the default PCST algorithm optimizes (low edge costs and
high vertex values).

Take the absolute value of the correlation so that the values range from low
correlation at 0 to high correlation at 1. Then, subtract all edge weight and
vertex values from 1. The graph is now an instance of a PCST and can be solved
by the algorithm.


## b)
*In PCST, we combine vertex and edge scores by choosing a relative weight that
balances the two types of evidence. For example, if we put too much weight on
edge scores, we may find a subnetwork with many co-expressed genes but no
disease relevance. Discuss how you may choose this relative weight.*

Approach 1: Scale the vertices based on the correlations.

One can take the adjacency matrix of genes, where each entry is the correlation
measured between each pair, and compute its eigenvectors. The matrix should be
transformed beforehand by taking the absolute values of correlations (strong
negative correlations are just as important as strong positive ones). The edges
in the matrix could represent connections known *a priori*, or the matrix could
be allowed to be strongly connected, where most edges are weighted near 0 due
to weak correlations. The resulting eigenvector will contain loadings for each
vertex that signify its importance to maximizing 'flow' through the network,
which can then be used to scale the p-values of the vertices before solving the
PCST.

This does have the drawback of there likely being a highly modular structure to
the gene regulatory network, which will cause the principal eigenvector to
heavily weigh large significant subgraphs and ignore small significant
subgraphs. Smaller significant subgraphs will be captured by other
eigenvectors. One strategy here could be to linearly combine eigenvectors with
eigenvalues above an arbitrary cutoff and use the resultant weight per-vertex
to scale the p-values in the graph.

Note that the p-values will have to be adjusted before running the PCST
algorithm, which prioritizes high values over small ones. One could subtract
the values from 1 or use the strategy outlined below.


Approach 2: Transform the range of values.

Absolute values of correlations range from $[0,1]$, and so do p-values from
$[0,1]$. To make differences between weak and strong correlations more stark,
one can transform these values to a larger space. For example, one could
transform p-values using an inverse sigmoid function so that p-values close to
1 will tend to negative infinity, and values close to 0 will tend to positive
infinity. This transformation would prevent selection of subgraphs via PCST
with elements without low p-values.


## c)
*PCST only returns the best tree. But for many complex disease, one would
expect to have multiple disease related gene modules. How would you address
this problem?*

If you make the assumption that the gene modules are orthogonal to one another
in that they do not share components, one can run the PCST algorithm for the
best global path and then remove the nodes (genes) in the graph. A less extreme
option could be to remove the edges corresponding to the minimum path. Then,
run the PCST algorithm again and repeat until no significant modules are found.

Another strategy could use eigenvalue decomposition of the network to pick out
important nodes in the network by looking at how vertices are weighed in each
eigenvector, assuming the eigenvalues are not very low.
